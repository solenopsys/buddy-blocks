# Производительность io_uring-потоков

## Что наблюдаем
- На локальной машине при 4 KB блоках получаем порядка 3.9–4.6 k запросов в секунду (`~15–18 MB/s`). Для 16 KB и 64 KB блоков пик держится в районе 35–40 MB/s: рост размера блока перестаёт помогать уже после ~32 KB.
- Латентность на GET при такой нагрузке 4–6 мс, при увеличении `-concurrency` растёт до 20 мс+, а пропускная способность по байтам почти не меняется.

## Почему скорость упирается в ~40 MB/s
- Потоки чтения/записи реализованы в `src/file_controller.zig:235-360`. Буфер всегда 4 KB (`BUFFER_SIZE = 4096`), и для каждого чанка выполняются два последовательных шага: `ring.read(...)` → `submit()` → `copy_cqe()`, затем `ring.write(...)` → `submit()` → `copy_cqe()`. Очередь io_uring каждый раз опустошается до глубины 0.
- Даже при больших блоках цикл всё равно раскладывает данные на 4 KB куски и обрабатывает их строго последовательно. Это приводит к одинаковой «стоимости» одного чанка вне зависимости от общего размера блока, поэтому MB/s зависает на плато.
- Сервер запускается в варианте `server_single.zig`, то есть один поток событий. Пока он ждёт `copy_cqe()`, никакая другая работа (хоть с сокетом, хоть с файлом) не выполняется.
- Дополнительные факторы: `O_DIRECT` на файле (динамика зависит от выравнивания) и постоянные SHA-256/LMDB операции перед каждой отправкой.

## Что делать
1. Увеличить размер рабочих буферов и реально передавать крупные куски за одно `read`/`write`, а не резать на 4 KB.
2. Батчить запросы: подготавливать несколько SQE подряд, звать `submit()` раз в N операций и разбирать CQE в отдельном цикле, сохраняя глубину очереди хотя бы 8–16.
3. Рассмотреть переход на многопоточный сервер (`server.zig`) и/или выделенный io_uring на поток, чтобы чтение/запись не блокировали обработку других клиентов.
4. Если цель — быстрый базовый тест, временно использовать классические `read`/`write`; при текущем синхронном паттерне эффекта от io_uring всё равно нет.
5. Вести координацию активных диапазонов в самом контроллере: BuddyAllocator уже защищён мьютексом, туда можно добавить отметки «offset занят», чтобы несколько потоков с собственными rings не писали в один участок. Это устранит необходимость в тяжёлых kernel-lock’ах и сохранит габарит мьютекса (мы и так заходим туда ради LMDB).
6. После перехода на concurrency имеет смысл переключить контроллер с «один блок – одна транзакция» на батчи. Резервируем пачку блоков под один `lmdbx`-коммит, выдаём worker’ам их offset’ы, копим журнал успехов/ошибок и фиксируем изменения в одной транзакции (или откатываем). Такой «debounce» раз в 10–50 µs позволит сократить число `allocate/free` вызовов и не превращать LMDBX в узкое горлышко, когда появится реальная параллельность.

## Прогноз после доработок
- При буферах 64–256 KB и глубине очереди ≥8 мы можем держать по 4–8 IO запросов на поток. На NVMe даже локально это обычно даёт 200–400 MB/s; на SATA — 80–120 MB/s. На loopback через `io_uring + tcp` реальная пропускная способность упрётся в стек/копирование, но ожидаемо выйти на 10–20 k rps с блоками 4–16 KB (40–60 MB/s) и выше 200 MB/s на крупных блоках вполне реально.
- Ключевой риск — LMDB. Он остаётся общим «бутылочным горлышком»; после увеличения IO пропускной способности может потребоваться tune транзакций (batched commits, отдельный writer-thread).
