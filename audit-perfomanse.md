# Аудит производительности Buddy Blocks

## Контекст
- Цель: понять, почему HTTP/RPC нагрузка упирается в ~3 500 PUT/GET операций в секунду (≈16 МБ/с на 4 КиБ блоках).
- Под нагрузкой участвуют 4 worker-потока, контроллер, LMDBX и файловый контроллер с `io_uring`.

## Ключевые узкие места

- **Резервирование блоков через `allocate_block` не используется в реальных PUT**  
  Фаза пополнения пулов (`src/worker/worker.zig:259-347`) посылает `allocate_block`, контроллер выделяет блок и пишет “временный” хеш в LMDBX (`src/controller/handler.zig:31-69`). Однако при фактическом PUT worker сразу же шлёт `occupy_block`, а контроллер снова вызывает `BuddyAllocator.allocate` с *новым* хешем (`src/controller/handler.zig:58-69`). Пул блоков не выдаёт prefetch-ресурс через `acquire`, и временные записи не заменяются. Итог:
  - двойное выделение с двумя LMDB транзакциями на каждый PUT;
  - “резервные” блоки остаются занятыми фиктивными хешами, что заставляет аллокатор постоянно расширять файл и ведёт к росту базы без реальной пользы;
  - увеличение латентности контроллера и лишняя работа LMDBX.  
  **Рекомендация:** либо удалить текущий prefetch-поток, либо довести его до конца: worker должен `acquire` блок из пула, а контроллер — переводить уже зарезервированный `block_num` на реальный хеш без повторного allocate/commit.

- **Каждый `occupy_block` обрабатывается отдельной транзакцией LMDB**  
  В `BuddyAllocator.allocate` для каждого запроса берётся глобальный `std.Thread.Mutex`, затем начинается и коммитится транзакция (`buddy_allocator/src/buddy_allocator.zig:103-137`). В батчинге контроллера (`src/controller/controller.zig:103-148`) запросы обрабатываются строго последовательно в одном потоке, без объединения в общую транзакцию. На высоком RPS это превращается в узкое место дискового журнала даже с `MDBX_SAFE_NOSYNC`, что объясняет плато около 3–4 тыс. оп/с.  
  **Рекомендации:** сгруппировать батч `occupy_requests` в одну read-write транзакцию, убрать лишний `mutex` (контроллер уже однопоточный) и/или рассмотреть асинхронный commit с буферизацией нескольких операций перед жёстким fsync.

- **Worker отвечает клиенту блокирующим `posix.write`**  
  Ответы на PUT/DELETE и заголовки GET отправляются через синхронные вызовы (`src/worker/worker.zig:647-668`, `src/worker/worker.zig:749-799`). При нагрузке с сотнями keep-alive соединений одна блокировка TCP-сокета «подвешивает» весь worker, обнуляя выигрыш от `io_uring`.  
  **Рекомендации:** перейти на `io_uring.send/sendmsg`/`write` SQE для всех сетевых отправок или хотя бы выставить сокеты в non-blocking и обслуживать их через кольцо.

- **Лишние копирования памяти в PUT-пути**  
  Тело запроса сначала накапливается в динамической `ArrayList`, затем целиком копируется в новый буфер перед `io_uring.write` (`src/worker/worker.zig:322-343`). Для 4 КиБ блоков это удваивает память/CPU, а для больших блоков (до 512 КиБ) даёт заметный overhead и давление на allocator.  
  **Рекомендации:** использовать заранее выделенные/переиспользуемые буферы, зарегистрированные буферы `io_uring` или стримить данные сразу в файл, не копируя в промежуточный массив.

- **Регулятор пауз контроллера ограничивает RPS при средней нагрузке**  
  `PauseRegulator` стартует с паузой 1 мс и переключает её на 0 только если за последнюю секунду пришло ≥1000 сообщений (`src/controller/pause_regulator.zig:4-55`). При сценариях с умеренным параллелизмом (до 1000 RPC/с) контроллер искусственно засыпает, удерживая потолок <1000 запросов/с и замедляя прогрев до высоких скоростей.  
  **Рекомендации:** уменьшить стартовую паузу, пересмотреть пороги/гистерезис и обновлять метрику чаще, чтобы не вешать жёсткий sleep при реальной работе.

## Дополнительные наблюдения

- `SpscQueue`-обёртки постоянно аллоцируют при пополнении `ArrayList` в `BatchController.collectMessages`, что при больших очередях вызывает лишние realloc — можно заранее `ensureTotalCapacity`.  
- `createNewMacroBlock` всегда зовёт `fallocate` на 1 МиБ; из-за описанной выше утечки резервных блоков эти вызовы происходят гораздо чаще, чем необходимо.  
- При `O_DIRECT` буферы `allocator.alloc(u8, body.len)` не гарантируют выравнивание на 4096, что потенциально ведёт к скрытым копиям/ошибкам ядра.

## Рекомендуемые шаги
1. Перепроектировать путь prefetch/occupy, чтобы фактически переиспользовать заранее выделенные блоки и не плодить фиктивные записи в LMDBX.  
2. Избавиться от транзакции на каждый PUT: батчево обрабатывать `occupy_requests` и откладывать commit либо использовать write-ahead очередь.  
3. Перевести отправку ответов и заголовков на неблокирующие `io_uring` операции.  
4. Убрать двойные копирования тела запроса и внедрить пул выровненных буферов.  
5. Ослабить агрессивный sleep в `PauseRegulator`, чтобы контроллер не ограничивал throughput на умеренных нагрузках.

## Оценка целевой производительности
- **NVMe, 1 worker, блок 4 КиБ:** после устранения указанных узких мест реальная отдача должна находиться в диапазоне 12 000 – 15 000 запросов в секунду (≈50 – 60 МБ/с) при условии, что диск выдерживает последовательные записи и LMDBX настроен на relaxed durability (аналогично `MDBX_SAFE_NOSYNC`).
- **NVMe, 8 workers, блок 4 КиБ:** при линейном масштабировании и отсутствии глобальных блокировок ожидаем 80 000 – 100 000 запросов в секунду (≈320 – 400 МБ/с); фактическое значение зависит от распределения CPU по NUMA и эффективности параллельного доступа к LMDBX/файлу.
